---
title: 3.2 메모리
---
# 3.2.1 메모리 계층
메모리 계층은 레지스터, 캐시, 메모리, 저장장치로 구성되어 있다.
## 레지스터
* CPU 안에 있는 작은 메모리, 휘발성, 속도 가장 빠름, 기억 용량이 가장 적다.
  
  CPU는 필요한 데이터를 메모리에서 가져와 레지스터에 저장하고 산술 논리 연산장치를 이용해 연산한 후, 그 결과를 다시 레지스터에 저장했다가 메모리로 옮긴다. 이러한 레지스터는 사용자 프로그램에 의해 변경되기 때문에 <u>사용자 가시 레지스터(User-visible Register)</u> 라고 하며, 아래의 레지스터가 이에 해당한다.
  * 데이터 레지스터(DR; Data Register): 메모리에서 가져온 데이터를 임시로 보관할 때 사용한다. CPU에 있는 대부분의 레지스터가 데이터 레지스터이기 때문에 일반 레지스터 혹은 범용 레지스터라고 부른다.
  * 주소 레지스터(AR; Address Register): 데이터 또는 명령어가 저장된 메모리의 주소는 주소 레지스터에 저장된다.
  
  DR, AR 외에 특별한 용도로 사용되는 레지스터도 있는데 이를 특수 레지스터라고 한다. 특수 레지스터는 사용자가 임의로 변경할 수 없기 때문에 <u>사용자 불가시 레지스터(user-invisible register)</u> 라고 부른다. 
  * 프로그램 카운터(PC; Program Counter): 다음에 실행할 명령어의 주소를 기억하고 있다가 제어장치에 알려준다. 다음에 실행할 명령어의 주소를 가리키기 때문에 프로그램 카운터를 명령어 포인터(instruction pointer)라고도 한다.
  * 명령어 레지스터(IR; instruction Register): 현재 실행 중인 명령어를 저장한다. 제어장치는 명령어 레지스터에 있는 명령을 해석한 후 외부 장치에 적절한 제어 신호를 보낸다.
  * 메모리 주소 레지스터(MAR; Memory Address Register): 메모리에서 데이터를 가져오거나 반대로 메모리로 데이터를 보낼 때 주소를 지정하기 위해 사용한다.
  * 메모리 버퍼 레지스터(MBR; Memory Buffer Register): 메모리에서 가져온 데이터나 메모리로 옮겨 갈 데이터를 임시로 저장한다. 항상 메모리 주소 레지스터와 함께 동작한다.
## 캐시
*  속도가 빠른 장치와 느린 장치 사이에서 속도차에 따른 병목 현상을 줄이기 위한 범용 메모리이다.
*  L1, L2 캐시를 지칭한다. 휘발성, 속도 빠름, 기억 용량이 적다. 참고로 L3 캐시도 있다.
   *  L1 캐시는 일반적으로 CPU 칩안에 내장되어 데이터 사용/참조에 가장 먼저 사용된다. 8~64KB 정도의 용량으로 CPU가 가장 빠르게 접근하게 되며, 여기서 데이터를 찾지 못하면 L2, L3 캐시 메모리로 넘어간다.
   *  L2 캐시 메모리는 용도와 역할은 L1과 비슷하지만 속도는 비교적 느리다. 일반적으로 64KB~4MB 정도가 사용되며, CPU 회로판에 별도에 칩으로 내장된다. L2 캐시는 L1 보다는 느리지만, 일반 메모리(RAM)보다는 빠르다.
   *  L3 캐시 메모리도 동일한 원리. 웬만한 프로세서에는 L3 캐시 메모리를 달고 있지 않는다. Intel core2 duo나 quad에는 L3 캐시가 없지만, 코어 i7에는 8MB를 달아뒀다. L1/L2 캐시 메모리 정도만 CPU 성능에 직접 영향을 미치기에 L3 캐시는 메인보드에 내장되어 있는 경우가 더 많아 크게 신경쓰지 않는다.

  
<b>지역성의 원리</b><br>
  현재 위치에 가까운 데이터가 멀리 있는 데이터보다 사용될 확률이 더 높다는 이론.
  <br>시간 지역성: 최근 사용한 데이터에 다시 접근하려는 특성을 말한다.
  <br>공간 지역성: 최근 접근한 데이터를 이루고 있는 공간이나 그 가까운 공간에 접근하는 특성을 말한다.

### 캐시히트와 캐시미스
캐시에서 원하는 데이터를 찾았다면 캐시히트, 해당 데이터가 캐시에 없다면 주메모리로 가서 데이터를 찾아오는 것을 캐시미스라고 한다.

![cacheHit_cacheMiss](https://drive.google.com/uc?export=view&id=18rX0yYQpiPGOMQgTx50N05ieJOC5-bwF)
    
위 그림처럼 캐시히트를 하게 되면 해당 데이터를 제어장치를 거쳐 가져오게 된다. 캐시히트의 경우 위치도 가깝고 CPU 내부 버스를 기반으로 작동하기 때문에 빠르다.<br>
반면에 캐시미스가 발생되면 메모리에서 가져오게 되는데, 이는 시스템 버스를 기반으로 작동하기 때문에 느리다.
    
캐시매핑: 캐시가 히트되기 위해 매핑하는 방법을 말하며, CPU의 레지스터와 주 메모리(RAM) 간의 데이터를 주고받을 때를 기반으로 설명한다.

**즉시 쓰기와 지연 쓰기**

* 즉시 쓰기(write through): 캐시에 있는 데이터가 변경되면 이를 즉시 메모리에 반영하는 방식. 메모리와의 빈번한 데이터 전송으로 인해 성능이 느려진다는 것이 단점이지만, 메모리의 최신 값이 항상 유지되기 때문에 급작스러운 정전에도 데이터를 잃어버리지 않는다.
* 지연쓰기(write back): 캐시에 있는 데이터가 변경되면 이를 즉시 메모리에 반영하는 것이 아니라 변경된 내용을 모아서 주기적으로 반영하는 방식. 카피백(copy back)이라고도 한다. 메모리와 캐시 된 데이터 사이의 불일치가 발생할 수도 있다는 것이 단점이지만, 메모리와의 데이터 전송 횟수가 줄어들어 시스템의 성능을 향상할 수 있다.

**웹 브라우저의 캐시**
사용자의 커스텀한 정보나 인증 모듈 관련 사항들을 웹 브라우저에 저장해 추후 서버에 요청할 때 자신을 나타내는 아이덴티티나 중복 요청 방지를 위해 쓰인다.
* 쿠키: 만료기한이 있는 키-값 저장소. 4KB까지 데이터를 저장할 수 있고 만료기한을 정할 수 있다.
* 로컬 스토리지: 만료기한이 없는 키-값 저장소. 10MB까지 저장할 수 있고, 웹 브라우저를 닫아도 유지되고 도메인 단위로 저장, 생성된다.
* 세션 스토리지: 만료기한이 없는 키-값 저장소. 탭 단위로 세션 스토리지를 생성, 탭을 닫을 때 해당 데이터가 삭제됩니다. 5MB까지 저장이 가능하다.

**데이터베이스의 캐싱 계층**<br>
데이터베이스 시스템을 구축할 때도 메인 데이터베이스 위에 레디스 데이터베이스 계층을 '캐싱 계층'으로 둬서 성능을 향상시키기도 한다.

## 주기억장치
*  RAM을 가리킨다. 휘발성, 속도 빠름, 기억 용량이 보통이다.<br>
  RAM은 휘발성과 비휘발성으로 나뉜다.
 
![Memory](https://drive.google.com/uc?export=view&id=16X4-1OkyHTp3c70oBLP6e5FN5dx53Yu-)

  * 휘발성 메모리<br>
  DRAM(Dynamic RAM)은 저장된 0과 1의 데이터가 일정 시간이 지나면 사라지므로 일정 시간마다 다시 재생시켜야 한다.<br>
  SRAM(Static RAM)은 전력이 공급되는 동안에는 데이터를 보관할 수 있어 재생할 필요가 없다. 따라서 속도는 빠르지만 가격이 비싸다.<br>
  일반적으로 메인메모리에는 DRAM을 사용하고, 캐시 같은 고속 메모리에는  SRAM을 사용한다.<br>
  SDRAM(Synchronous Dynamic Random Access Memory)은 클록틱(펄스)이 발생할 때마다 데이터를 저장하는 동기 DRAM이다. SRAM과는 완전히 다른 종류의 램이다.
  * 비휘발성 메모리<br>
  플래시 메모리는 디지털 카메라, MP3 플레이어, USB 드라이브같이 전력이 없어도 데이터를 보관하는 저장장치로 많이 사용된다. <br>
  FRAM과 PRAM은 차세대 메모리이다. 차세대 반도체이며, DRAM이 가진 고속/고집적도성과 플래시 메모리의 비휘발성 장점을 모두 갖추어 향후 모든 종류의 반도체를 대체할 수도 있을 것으로 예상된다.
## 보조기억장치
*  HDD, SSD를 일컬으며 비휘발성, 속도 낮음, 기억 용량이 많다.
# 3.2.2 메모리 관리
## 가상 메모리
물리 메모리의 크기와 상관없이 프로세스에 커다란 메모리 공간을 제공하는 기술이다.<br>
물리 메모리의 크기가 어느 정도인지 신경 쓰지 않고 메모리를 마음대로 사용할 수 있다.
![logical_memory](https://drive.google.com/uc?export=view&id=1zaZMt02xFe4j0hMXzUrcWs5MNXZjjImF)
가상적으로 주어진 주소를 가상 주소(logical address)라고 하며, 실제 메모리상에 있는 주소를 실제 주소(physical address)라고 한다. 가상 주소는 메모리관리장치(MMU)에 의해 실제 주소로 변환되며, 따라서 사용자는 실제 주소를 의식할 필요 없이 프로그램을 구축할 수 있다.<br>
가상 메모리는 가상 주소와 실제 주소가 매핑되어 있고 프로세스의 주소 정보가 들어 있는 '페이지 테이블'로 관리된다. 이때 속도 향상을 위해 사용되는 것이 TLB이다.
> TLB<br>
> 메모리와 CPU 사이에 있는 주소 변환을 위한 캐시. 페이지 테이블 리스트를 보관하며 CPU가 페이지 테이블까지 가지 않도록 해 속도 향상에 도움을 준다. 

**스와핑(Swaping)**<br>
하드디스크의 일부분을 마치 메모리처럼 불러와 쓰는 것을 스와핑이라고 한다. 원래 하드디스크 같은 저장장치는 저장장치 관리자가 관리하나, 스와핑은 메모리 관리자가 관리한다.<br>
스왑영역은 <u>메모리에서 쫓겨났다가 다시 돌아가는 데이터가 머무는 곳이기 때문에</u> 저장장치는 장소만 빌려주고 메모리 관리자가 관리하는 것이다. 때문에 스왑영역은 가상 메모리의 구성 요소 중 하나이다.<br>
스왑 영역에서 메모리로 데이터를 가져오는 작업은 스왑인, 메모리에서 스왑 영역으로 데이터를 내보내는 작업은 스왑아웃이라고 한다.
![swaping](https://drive.google.com/uc?export=view&id=1m3USFfPLniP8OCTlh0-5WVAd0hSmt1La)

 이를 통해 마치 페이지 폴트가 일어나지 않은 것처럼 한다.

**페이지 폴트(Page Fault)**<br>
페이지 부재라고도 하며, 프로세스가 페이지를 요청했을 때 그 페이지가 메모리에 없는 상황을 말한다. 프로세스가 해당 페이지를 사용할 수 있도록 스왑 영역에서 물리 메모리로 옮겨야 한다.<br>
*처리 과정*
1. CPU는 물리 메모리를 확인해 해당 페이지가 없으면 트랩을 발생해서 운영체제에 알린다.
2. 운영체제는 CPU의 동작을 잠시 멈춘다.
3. 운영체제는 페이지 테이블을 확인해 가상 메모리에 페이지가 존재하는지 확인, 없으면 프로세스를 중단하고  현재 물리 메모리에 비어있는 프레임이 있는지 찾는다. 물리 메모리에도 없으면 이 때 스와핑이 발생한다.
4. 비어 있는 프레임에 해당 페이지를 로드하고, 페이지 테이블을 최신화한다.
5. 중단되었던 CPU를 다시 실행한다.
> 페이지(Page): 가상 메모리를 사용하는 최소 크기 단위<br>
> 프레임(Frame): 실제 메모리를 사용하는 최소 크기 단위

프로세스 단위로 스와핑을 하게 되면 외부 단편화 문제가 발생하며, 이는 비효율적이다.<br>
프로세스는 필요한 부분만 메모리에 올려서 실제로 메모리에 올라가는 프로세스의 크기를 최소화 시키면 된다.<br>
해결책으로 페이징 단위로 스와핑을 실행하면 된다. 이를 요구 페이징(Demand Paging)이라고 한다.<br>

**요구 페이징(Demand Paging)**<br>
사용자가 요구할 때 해당 페이지를 메모리로 가지고 오는 것이다.<br>
프로그램 일부만 가져와 실행하고 사용자가 특정 기능을 요구할 때 해당 모듈을 메모리에 올리면 메모리 절약, 메모리의 효율적 관리, 프로세스의 응답 속도 향상 등의 효과를 볼 수 있다.

### 스레싱
하드 디스크의 입출력이 너무 많아져서 잦은 페이지 부재로 작업이 멈춘 것 같은 상태

**스레싱 발생 시점**<br>
* CPU가 작업하는 시간보다 스왑 영역으로 페이지를 보내고 새로운 페이지를 메모리에 가져오는 작업이 빈번해져서 CPU가 작업할 수 없는 상태에 이르게 되는 시점.
* 물리 메모리의 크기를 늘리면 스레싱 발생 지점이 늦춰져서 프로세스를 원만하게 실행할 수 있다.

**스레싱과 프레임 할당**
* 프로세스에 너무 적은 프레임을 할당하면 페이지 부재가 빈번히 일어난다.
* 프로세스에 너무 많은 프레임을 할당하면 페이지 부재는 줄지만 메모리가 낭비된다.
* 프로세스에 프레임을 할당하는 방식은 크게 연속 할당과 불연속 할당으로 구분한다.

**작업 세트(Working Set)**<br>
프로세스의 과거 사용 이력인 지역성(locality)을 통해 결정된 페이지 집합을 만들어서 미리 메모리에 로드하는 것이다.
**PFF(Page Fault Frequency)**
페이지 폴트 빈도를 조절하는 방법, 상한선과 하한선을 만드는 방법이다. 즉, 페이지 폴트가 자주 일어나면 메모리를 더 주고, 자주 일어나지 않으면 메모리를 뺏는다.
### 메모리 할당
**연속 할당**<br>
메모리에 <u>연속적으로</u> 공간을 할당하는 방식이다.<br> 

고정 분할 방식(fixed partition allocation)<br>
* 메모리를 미리 나누어 관리하는 방식.<br>
* 메모리가 미리 나뉘어 있어 융통성이 없고 내부 단편화가 발생한다.

가변 분할 방식<br>
* 매 시점 프로그램의 크기에 맞게 동적으로 메모리를 나눠 사용한다. 외부 단편화만 발생할 수 있다.

> 내부 단편화: 메모리의 빈 공간에 프로세스가 들어갔을 때 남은 공간이 너무 작은 경우를 말한다.<br>
> 외부 단편화: 메모리에 남아 있는 공간은 충분하지만, 프로세스의 크기가 너무 커 메모리에 들어가지 못하는 경우를 말한다.

**불연속 할당**<br>

페이징<br>
동일한 크기의 페이지 단위로 나누어 메모리의 서로 다른 위치에 프로세스를 할당한다.<br>
물리 주소 공간을 같은 크기로 나누어 사용한다.<br>
가상 주소는 프로세스 입장에서 바라본 메모리 공간으로 항상 0번지부터 시작한다.
> **페이징의 주소 변환**<br>
> 가상 주소: VA=<P,D><br>
> VA: 가상 주소<br>
> P: 페이지<br>
> D(distance): 페이지의 처음 위치에서 해당 주소까지의 거리
> 물리 주소: PA=<F,D><br>
> PA: 물리 메모리의 주소를 가리키는 용어로 물리 주소 또는 실제 주소<br>
> F: 프레임<br>
> D: 프레임의 처음 위치에서 해당 주소까지의 거리<br>
> VA=<P,D> ➡️ PA=<F,D><br>
> 페이지 테이블을 사용하여 P는 F로 바꾸고 D는 변경 없이 그대로 쓴다.
> D를 변경하지 않는 이유는 페이지와 프레임의 크기를 똑같이 나누었기 때문이다.
>
> *가상 주소를 <P,D>로 변환하는 공식*<br>
> P = 나눗셈(가상 주소/한 페이지의 크기)의 몫<br>
> D = 나눗셈(가상 주소/한 페이지의 크기)의 나머지

세그멘테이션<br>
페이지 단위가 아닌 의미 단위인 세그먼트(segment)로 나누는 방식이다.<br>
세그먼트의 크기를 나타내는 limit와 물리 메모리상의 시작 주소를 나타내는 address가 있다.<br>
각 세그먼트가 자신에게 주어진 메모리 영역을 넘어가면 안 되기 때문에 세그먼트의 크기 정보에는 크기를 뜻하는 size 대신 제한을 뜻하는 limit를 사용한다.<br>
세그먼테이션 기법에서도 물리 메모리가 부족할 때 스왑 영역을 사용한다.

페이지드 세그멘테이션<br>
프로그램을 의미 단위인 세그먼트로 나눠 공유나 보안 측면에서 강점을 두고 임의의 길이가 아닌 동일한 크기의 페이지 단위로 나누는 것을 말한다.
### 페이지 교체 알고리즘
**오프라인 알고리즘**<br>
먼 미래에 참조되는 페이지와 현재 할당하는 페이지를 바꾼다. 하지만 미래의 프로세스를 알 수 없다.<br>
사용할 수 없지만 가장 좋은 알고리즘이기 때문에 성능 비교에 대한 상한 기준(upper_bound)을 제공한다.

**FIFO**<br>
시간상으로 메모리에 가장 먼저 들어온 페이지를 대상 페이지로 선정하여 스왑영역으로 쫓아낸다.<br>
메모리가 꽉 차면 맨 위의 페이지가 스왑 영역으로 가고 나머지 페이지들이 위쪽으로 이동하며, 새로운 페이지가 아래쪽의 남은 공간에 들어온다.<br>
무조건 오래된 페이지를 대상 페이지로 선정하기 때문에 성능이 떨어진다.

**FIFO 변형**<br>
FIFO와 마찬가지로 큐를 사용한다.
특정 페이지에 접근하여 페이지 부재 없이 성공할 경우 해당 페이지를 큐의 맨 뒤로 이동하여 대상 페이지에서 제외한다.<br>
성공한 페이지를 큐의 맨 뒤로 옮김으로써 기회를 한 번 더 준다.

**LRU**<br>
<u>최근 최소 사용 페이지 교체 알고리즘</u>이라고도 한다.<br>
메모리에 올라온 후 가장 오랫동안 사용되지 않은 페이지를 스왑 영역으로 옮긴다.<br>
최근에 사용된 페이지는 놔두고 오래전에 사용된 페이지를 대상 페이지로 선정한다.<br>
알고리즘은 시간을 기준으로 구현할 수 있으며(clock algorithm) 카운터나 참조 비트를 이용하는 방법도 있다.

**LFU**<br>
가장 참조 횟수가 적은 페이지를 교체한다.<br>
페이지가 몇 번 사용 되었는지를 기준으로 대상 페이지를 선정한다.<br>
현재 프레임에 있는 페이지마다 그 동안 사용된 횟수를 세어 횟수가 가장 적은 페이지를 스왑 영역으로 옮긴다.<br>
**NUR(Not Used Recently)**<br>
‘최근 미사용 페이지 교체 알고리즘’이라고도 불린다.<br>
LRU, LFU 페이지 교체 알고리즘은 성능은 좋으나 추가적인 오버헤드가 크다.<br>
이를 개선한 NRU는 두 개의 비트만으로 구현 가능하다.<br>
페이지마다 참조 비트와 변경 비트를 가진다.
> 참조 비트 : 페이지에 접근(read/execute)하면 1이 됨<br>
> 변경 비트 : 페이지가 변경(write/append)되면 1이 됨
모든 페이지의 초기 상태는 (0,0), 모든 값이 (1,1)이면 초기화 한다.


> 참조
> 
> https://resilient-923.tistory.com/397
> 
> https://12bme.tistory.com/402